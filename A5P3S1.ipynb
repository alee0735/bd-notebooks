{"cells":[{"cell_type":"code","source":["# Load data\ndataDF = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"delimiter\", \",\").option(\"header\", True).load(\"/FileStore/tables/risk_factors_cervical_cancer.csv\")"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["display(dataDF)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["from pyspark.sql.functions import when, lit, col\n# Majority of rows are missing values for these columns so we drop them\ndataDF = dataDF.drop(\"STDs: Time since first diagnosis\").drop(\"STDs: Time since last diagnosis\").drop('Hinselmann').drop('Schiller').drop('Citology')"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Remove all rows with questions marks in them\nfor c in dataDF.schema.names:\n  dataDF = dataDF.filter(~col(c).isin(['?']))\n\n# Convert all data from string to float\nfrom pyspark.sql.types import DoubleType\n\nfor c in dataDF.schema.names:\n  dataDF = dataDF.withColumn(str(c), dataDF[str(c)].cast(DoubleType()))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import StandardScaler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer\n\n#\ndataDF = dataDF.withColumnRenamed(\"Biopsy\", \"label\")\n#\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\")\nvecAssembler = VectorAssembler(inputCols=dataDF.schema.names, outputCol=\"features\")\nlr = LogisticRegression(labelCol=\"indexedLabel\", featuresCol=\"features\")\npipeline = Pipeline(stages=[vecAssembler, labelIndexer, lr])\n#maxIter=10"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nparamGrid = ParamGridBuilder().addGrid(lr.maxIter, [10, 50, 100]).addGrid(lr.regParam, [0.1, 0.01, 0.001]).build()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\ncrossval = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=BinaryClassificationEvaluator(), numFolds=5)  "],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["dataDF.count()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["splits = dataDF.randomSplit([0.8, 0.20], 10)\ntraining = splits[0]\ntest = splits[1]"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["training.schema.names"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["cvModel = crossval.fit(training)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Make predictions.\npredictions = cvModel.transform(test)\npredictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g \" % (1.0 - accuracy))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["from pyspark.mllib.evaluation import MulticlassMetrics\npredictionsAndLabels = predictions.select(\"prediction\", \"label\").rdd\nmetrics = MulticlassMetrics(predictionsAndLabels)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Overall statistics\nprecision = metrics.precision()\nrecall = metrics.recall()\nf1Score = metrics.fMeasure()\nprint(\"Summary Stats\")\nprint(\"Accuracy = %s\" % accuracy)\nprint(\"Precision = %s\" % precision)\nprint(\"Recall = %s\" % recall)\nprint(\"F1 Score = %s\" % f1Score)\n\n# Weighted stats\nprint(\"Weighted recall = %s\" % metrics.weightedRecall)\nprint(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\nprint(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\nprint(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["from pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.feature import PCA\n\n# Model 2\npca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\nlr = LinearRegression(labelCol=\"indexedLabel\", featuresCol=\"pcaFeatures\")\npipeline = Pipeline(stages=[vecAssembler, labelIndexer,pca, lr])\ncvModel2 = crossval.fit(training)\npredictions = cvModel2.transform(test)\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g \" % (1.0 - accuracy))\n\npredictionsAndLabels = predictions.select(\"prediction\", \"label\").rdd\nmetrics = MulticlassMetrics(predictionsAndLabels)\n\n# Overall statistics\nprecision = metrics.precision()\nrecall = metrics.recall()\nf1Score = metrics.fMeasure()\nprint(\"Summary Stats\")\nprint(\"Precision = %s\" % precision)\nprint(\"Recall = %s\" % recall)\nprint(\"F1 Score = %s\" % f1Score)\n\n# Weighted stats\nprint(\"Weighted recall = %s\" % metrics.weightedRecall)\nprint(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\nprint(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\nprint(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)"],"metadata":{},"outputs":[],"execution_count":16}],"metadata":{"name":"A5P3S1","notebookId":1098628433114578},"nbformat":4,"nbformat_minor":0}
